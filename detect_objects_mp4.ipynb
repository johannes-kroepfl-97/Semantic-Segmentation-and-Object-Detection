{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49df0409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kroep\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Users\\kroep\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import copy\n",
    "import openai\n",
    "import numpy as np\n",
    "\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "746c9ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_deeplab_model():\n",
    "    \"\"\"\n",
    "    Load the DeepLab model.\n",
    "\n",
    "    Returns:\n",
    "        torch.model: The loaded DeepLab model.\n",
    "    \"\"\"\n",
    "    model = models.segmentation.deeplabv3_resnet101(pretrained=True)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def apply_semantic_segmentation(model, img):\n",
    "    \"\"\"\n",
    "    Apply semantic segmentation to the image using the DeepLab model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.model): The loaded DeepLab model.\n",
    "        img (ndarray): The image to segment.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: The segmented image.\n",
    "    \"\"\"\n",
    "    # define the transformations\n",
    "    trf = transforms.Compose([transforms.ToTensor(), \n",
    "                              transforms.Normalize(mean = [0.485, 0.456, 0.406], \n",
    "                                                   std = [0.229, 0.224, 0.225])])\n",
    "    \n",
    "    # apply transformations\n",
    "    inp = trf(img).unsqueeze(0)\n",
    "\n",
    "    # perform forward pass and get predictions\n",
    "    out = model(inp)['out']\n",
    "    om = torch.argmax(out.squeeze(), dim=0).detach().cpu().numpy()\n",
    "\n",
    "    # create RGB version of mask\n",
    "    r = np.zeros_like(om).astype(np.uint8)\n",
    "    g = np.zeros_like(om).astype(np.uint8)\n",
    "    b = np.zeros_like(om).astype(np.uint8)\n",
    "    r[om == 1] = 255\n",
    "    g[om == 0] = 255\n",
    "    segmented_image = np.stack([r, g, b], axis=2)\n",
    "\n",
    "    return segmented_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa7f8b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(image, detected_objects):\n",
    "    \"\"\"\n",
    "    Draw bounding boxes and labels on the image for detected objects.\n",
    "\n",
    "    Args:\n",
    "        image (ndarray): The image on which to draw bounding boxes and labels.\n",
    "        detected_objects (list): List of dictionaries, each containing label, confidence, and location of a detected object.\n",
    "    \"\"\"\n",
    "    for obj in detected_objects:\n",
    "        label = obj[\"label\"]\n",
    "        confidence = obj[\"confidence\"]\n",
    "        location = obj[\"location\"]\n",
    "        \n",
    "        # calculate bounding box coordinates\n",
    "        center_x, center_y = location[\"center\"]\n",
    "        width, height = location[\"width\"], location[\"height\"]\n",
    "        x = int(center_x * image.shape[1] - width * image.shape[1] / 2)\n",
    "        y = int(center_y * image.shape[0] - height * image.shape[0] / 2)\n",
    "        w = int(width * image.shape[1])\n",
    "        h = int(height * image.shape[0])\n",
    "        \n",
    "        color = (0, 255, 0)  # green\n",
    "        \n",
    "        # draw bounding box\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "        \n",
    "        # draw label with confidence score\n",
    "        text = f\"{label}: {confidence:.2f}\"\n",
    "        cv2.putText(image, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77d4b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = os.path.join('videos', 'video_1.mp4')\n",
    "output_path = os.path.join('videos', 'video_1_processed.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd9ad157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load YOLO model and output layers\n",
    "net, output_layers = load_yolo_model()\n",
    "\n",
    "# open video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# check if video capture was successful\n",
    "if not cap.isOpened():\n",
    "    print(\"Unable to open Video!\")\n",
    "    exit()\n",
    "\n",
    "# get frames per second and frame size\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# save processed video with VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_w, frame_h))\n",
    "\n",
    "# loop to process each frame in video\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # detect objects in frame\n",
    "    outs, width, height = detect_objects(net, output_layers, frame)\n",
    "    classes, colors = get_classes_colors()\n",
    "    detected_objects = process_detections(outs, width, height, classes, colors, frame)\n",
    "    \n",
    "    # draw bounding boxes and labels on frame\n",
    "    draw_boxes(frame, detected_objects)\n",
    "    \n",
    "    # display frame with detected objects and save it\n",
    "    cv2.imshow(\"Video\", frame)\n",
    "    cv2.waitKey(1)\n",
    "    out.write(frame)\n",
    "\n",
    "# release video capture and output writer objects\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27475c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
